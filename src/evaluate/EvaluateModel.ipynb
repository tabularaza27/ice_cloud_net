{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef484160-ae1c-4c75-bdff-4b1d7a012877",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d14586-c3aa-46ae-a6d8-8014857edd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b898d9-02cd-4aaa-9598-cc9df562eb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from datetime import datetime,date\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import einops\n",
    "from copy import deepcopy\n",
    "from pprint import pprint\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667db770-cbcb-4d51-9e36-80acf119f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b057e54d-775f-45a7-b28a-268f86ac0cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import omegaconf\n",
    "from omegaconf import OmegaConf\n",
    "import comet_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96a03d6-c19e-46c7-8c9d-9c9f3f85fecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88fb9c8-e6b7-4584-8edf-8641c55e9272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "sns.set_theme(style=\"ticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed8337-29e5-4d70-8a76-8f5104dd9ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c30b4f-628a-4f83-b3cf-bb1e4ebf433f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as mtransforms\n",
    "from matplotlib import colors, ticker\n",
    "from matplotlib.colors import LogNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7e6394-acc6-431a-a7cc-932c79c923b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d002f0e-8c0d-4a4b-b16b-9d9818d285a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bokeh import palettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493f9195-1a2e-43e9-9fda-3068657480a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import holoviews as hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a8b145-de72-4a5b-ae7e-a6e7b77ea07f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hvplot.pandas  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7037064b-0e23-4842-a5db-507683fe8538",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131f5e9e-a178-4ff7-b851-4cd4c70d0a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7604b5d5-3224-4574-9a1c-f884f449822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_module import VerticalCloudDataModule, LogTransform,LogTransform2D, get_variable_stat\n",
    "\n",
    "from data.data_utils import load_patches, sort_overpass_indices, get_overpass_direction\n",
    "\n",
    "from helpers.comet_helpers import get_patch_ids, load_experiment, get_hparams, get_trained_model, get_trained_experiment, get_dm, get_data_module, get_asset_id, dm_hparams_config, model_hparams_config, dm_overwrite_hparams, model_overwrite_hparams\n",
    "from helpers.callbacks import *\n",
    "\n",
    "from model.losses import *\n",
    "\n",
    "from model.LightningModel import VerticalCloudCubeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf27d68-217d-47b8-8a5f-2d392acc44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_utils import get_horizontal_cloud_coverage, horizontal_cover_by_level, UnNormalize, get_height_level_range\n",
    "from helpers.misc_helpers import  nested_set\n",
    "from helpers.comet_helpers import log_image_to_comet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2c2b4f-bbd4-4b14-8c02-0757a4d5a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate.eval_plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b40e39-f594-48a2-9220-dd5d68ed43e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.DiscriminatorModel import IceCloudNetDisc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2ac653-9e1c-4070-9b8a-8c9e0abe3daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference.inference import load_discriminator_model, load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58098b4-b6ff-485a-aa12-f8b2a1225041",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619b066b-7087-4667-8f27-12645bcff08b",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ee91dd-95dd-4941-b36f-ba9f69c57815",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_color = \"#30A2DA\"\n",
    "dardar_color = \"#FD654B\"\n",
    "\n",
    "hv.extension('matplotlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b508a520-462d-4e4b-ad05-7f44ec88d8fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### User defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e3264d-3647-4a59-9021-25b1eb3772dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hparams(experiment, hparams_config,overwrite_params,print_hparams=False):\n",
    "    hparams = dict()\n",
    "    for param in experiment.get_parameters_summary():\n",
    "        p_name = param[\"name\"]\n",
    "        if p_name not in hparams_config.keys():\n",
    "            continue\n",
    "        elif param[\"valueCurrent\"] == \"iwc\":\n",
    "            param_value = \"iwc\"\n",
    "        elif param[\"valueCurrent\"] in [\"None\",'null']:\n",
    "            param_value = None\n",
    "        elif hparams_config[p_name] in (list,tuple,dict):\n",
    "            param_value_str = param[\"valueCurrent\"]\n",
    "            param_value = json.loads(param_value_str)\n",
    "        elif hparams_config[p_name] == bool:\n",
    "            if param[\"valueCurrent\"] == \"false\":\n",
    "                param_value = False\n",
    "            else:\n",
    "                param_value = True\n",
    "        elif hparams_config[p_name] == datetime.date:\n",
    "            param_value = datetime.strptime(param[\"valueCurrent\"],\"%Y-%m-%d\").date()\n",
    "        elif hparams_config[p_name] in [int,float,str,np.float64]:\n",
    "            param_value = hparams_config[p_name](param[\"valueCurrent\"])\n",
    "        elif hparams_config[p_name] == LogTransform:\n",
    "            args = [\"constant\", \"scaler\"]\n",
    "            args_dict = dict()\n",
    "            input_string = param[\"valueCurrent\"]\n",
    "            for arg in args:\n",
    "                match = re.search(fr'{arg}=(\\d+\\.*)', input_string)\n",
    "                # Check if the pattern was found\n",
    "                if match:\n",
    "                    # Extract the matched value\n",
    "                    args_dict[arg] = np.float64(match.group(1))\n",
    "            param_value=LogTransform(**args_dict)\n",
    "    \n",
    "        else:\n",
    "            print(f\"No value for {p_name} available/possible to parse → set manually\")\n",
    "            param_value = \"SetManually\"\n",
    "        hparams[p_name] = param_value\n",
    "\n",
    "    # set params to overwrite\n",
    "    print(overwrite_params)\n",
    "    for key,val in overwrite_params.items():\n",
    "        print(f\"Set {key} manually with value {val}\")\n",
    "        hparams[key]=val\n",
    "        \n",
    "    if print_hparams:\n",
    "        print(\"-------\")\n",
    "        print(\"hparams\")\n",
    "        print(\"-------\")\n",
    "        pprint(hparams,depth=1)\n",
    "    return hparams\n",
    "\n",
    "def get_trained_model(experiment, model_hparams):\n",
    "    vertical_cloud_model = VerticalCloudCubeModel(**model_hparams)\n",
    "    exp_key = experiment.get_metadata()[\"experimentKey\"]\n",
    "    try:\n",
    "        # load local\n",
    "        path = glob.glob(os.path.join(\"/cluster/work/climate/kjeggle/model_checkpoints\", exp_key, \"*.ckpt\"))[0]\n",
    "        pretrained_state_dict = torch.load(path)[\"state_dict\"]\n",
    "        print(\"loaded model from disk\")\n",
    "    except IndexError:\n",
    "        # load from comet\n",
    "        asset_id = experiment.get_model_asset_list(\"ice_net\")[0][\"assetId\"]\n",
    "        model_binary = experiment.get_asset(asset_id, return_type=\"binary\")\n",
    "        pretrained_state_dict = torch.load(io.BytesIO(model_binary))\n",
    "        print(\"loaded model from comet\")\n",
    "    vertical_cloud_model.load_state_dict(pretrained_state_dict)\n",
    "    \n",
    "    return vertical_cloud_model\n",
    "\n",
    "def get_data_module(experiment, dm_hparams):\n",
    "    return VerticalCloudDataModule(**dm_hparams)\n",
    "\n",
    "def get_dm(experiment_id, dm_overwrite_params,dm_hparams_config=dm_hparams_config):\n",
    "    experiment = load_experiment(experiment_name=experiment_id)\n",
    "    print(\"loaded experiment from comet\")\n",
    "    \n",
    "    dm_hparams = get_hparams(experiment, \n",
    "                             print_hparams=True,\n",
    "                             hparams_config=dm_hparams_config,\n",
    "                             overwrite_params=dm_overwrite_params)\n",
    "    \n",
    "    dm = VerticalCloudDataModule(**dm_hparams)\n",
    "    dm.setup(stage=\"fit\")\n",
    "    \n",
    "    return dm \n",
    "\n",
    "def get_trained_experiment(experiment_id,\n",
    "                           model_overwrite_params,\n",
    "                           model_hparams_config=model_hparams_config):\n",
    "\n",
    "    experiment = load_experiment(experiment_name=experiment_id)\n",
    "    print(\"loaded experiment from comet\")\n",
    "       \n",
    "    model_hparams = get_hparams(experiment, \n",
    "                                hparams_config=model_hparams_config, \n",
    "                                overwrite_params=model_overwrite_params,\n",
    "                                print_hparams=False)\n",
    "    \n",
    "    model = VerticalCloudCubeModel(**model_hparams)\n",
    "\n",
    "    # load local\n",
    "    exp_key = experiment.get_metadata()[\"experimentKey\"]\n",
    "    try:\n",
    "        path = glob.glob(os.path.join(\"/cluster/work/climate/kjeggle/model_checkpoints\", exp_key, \"*.ckpt\"))[0]\n",
    "        pretrained_state_dict = torch.load(path)[\"state_dict\"]\n",
    "        print(\"loaded model from disk\")\n",
    "    except IndexError:\n",
    "        # load from comet\n",
    "        asset_id = experiment.get_model_asset_list(\"ice_net\")[0][\"assetId\"]\n",
    "        model_binary = experiment.get_asset(asset_id, return_type=\"binary\")\n",
    "        pretrained_state_dict = torch.load(io.BytesIO(model_binary))\n",
    "        print(\"loaded model from comet\")\n",
    "    model.load_state_dict(pretrained_state_dict)\n",
    "    \n",
    "    return experiment, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b27481-7f15-4427-8bd5-43170ee3b155",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0b64c5-2123-4b75-bc4e-d254c0132456",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(load_experiment(\"famous_elk_2849\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0fbdc8-2b45-4f1d-82bb-b99dd64b318a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Create Inference data (execute hidden cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c162d05e-5a03-468f-94f3-757de4d63e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inference_data(model, dataloader,store_3d_data = False,load_meta_data = True,split=\"val\",stop_at_step=10):\n",
    "\n",
    "    seviri_data = []\n",
    "    era5_data = []\n",
    "    overpass_mask_data = []\n",
    "    patch_idx_data = []\n",
    "    dardar_cube_data = []\n",
    "    y_hat_cube_data = []\n",
    "    y_hat_profile_data_list = []\n",
    "    dardar_profile_data_list = []\n",
    "    y_hat_cloud_cover_data = []\n",
    "    dardar_cloud_cover_data = []\n",
    "    meta_data = []\n",
    "\n",
    "    out_channels = model.out_channels\n",
    "    \n",
    "    # manually set level thickness for calculating horizontal cloud cover\n",
    "    if out_channels >= 256:\n",
    "        n_level_aggregation=16\n",
    "    elif out_channels == 64:\n",
    "        n_level_aggregation = 4\n",
    "    elif out_channels == 55:\n",
    "        n_level_aggregation = 5\n",
    "    elif out_channels == 16:\n",
    "        n_level_aggregation = 1\n",
    "    else:\n",
    "        raise ValueError(f\"n_level_aggregation for out channels {n_level_aggregation}\")\n",
    "        \n",
    "    if isinstance(model, IceCloudNetDisc):\n",
    "        n_targets = model.unet.prediction_heads\n",
    "    else:\n",
    "        n_targets = model.prediction_heads\n",
    "        \n",
    "    \n",
    "    for step, data in enumerate(dataloader):\n",
    "        if step % 10 == 0: print(step)\n",
    "        \n",
    "        if isinstance(model, IceCloudNetDisc):\n",
    "            seviri, era5, dardar, overpass_mask, md, patch_idx = model.get_input(data, split=\"val\")\n",
    "            seviri = seviri.float() # this means float32, double() is float64,\n",
    "            overpass_mask = overpass_mask.long()\n",
    "            dardar = dardar.float()\n",
    "        else:\n",
    "            seviri, era5, dardar, overpass_mask, md, patch_idx = data\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            seviri = seviri.to(torch.device(\"cuda\"))\n",
    "            md = md.to(torch.device(\"cuda\"))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if model.meta_data_embedding:\n",
    "                y_hat = model(seviri, md)\n",
    "            else:\n",
    "                y_hat = model(seviri)\n",
    "\n",
    "        y_hat = y_hat.cpu()\n",
    "        dardar = dardar.cpu()\n",
    "\n",
    "        # for each sample extract profile\n",
    "        for idx in range(y_hat.shape[0]):\n",
    "            y_hat_profile = torch.masked_select(y_hat[idx], overpass_mask[idx].bool())\n",
    "            y_hat_profile = einops.rearrange(y_hat_profile, '(c z overpass) -> overpass c z',c=n_targets, z=out_channels).squeeze() # squeeze so shape is (overpass, z) for single target\n",
    "            \n",
    "            dardar_profile = torch.masked_select(dardar[idx], overpass_mask[idx].bool())\n",
    "            dardar_profile = einops.rearrange(dardar_profile, '(c z overpass) -> overpass c z',c=n_targets, z=out_channels).squeeze()\n",
    "\n",
    "            y_hat_profile_data_list.append(y_hat_profile)\n",
    "            dardar_profile_data_list.append(dardar_profile)\n",
    "\n",
    "            # calculate horizontal cloud cover per level\n",
    "            if n_targets == 1:\n",
    "                dardar_cloud_cover = horizontal_cover_by_level(dardar_profile,n_level_aggregation=n_level_aggregation)\n",
    "                y_hat_cloud_cover = horizontal_cover_by_level(y_hat_profile,n_level_aggregation=n_level_aggregation)\n",
    "            else:\n",
    "                # use only single target\n",
    "                dardar_cloud_cover = horizontal_cover_by_level(dardar_profile[:,0],n_level_aggregation=n_level_aggregation)\n",
    "                y_hat_cloud_cover = horizontal_cover_by_level(y_hat_profile[:,0],n_level_aggregation=n_level_aggregation)\n",
    "                \n",
    "            dardar_cloud_cover_data.append(dardar_cloud_cover)\n",
    "            y_hat_cloud_cover_data.append(y_hat_cloud_cover)        \n",
    "\n",
    "        era5_data.append(era5.cpu())\n",
    "        overpass_mask_data.append(overpass_mask.cpu())\n",
    "        patch_idx_data.append(patch_idx.cpu())\n",
    "        seviri_data.append(seviri.cpu())\n",
    "        meta_data.append(md.cpu())\n",
    "\n",
    "        if store_3d_data:\n",
    "            dardar_cube_data.append(dardar.cpu())\n",
    "            y_hat_cube_data.append(y_hat.cpu())\n",
    "\n",
    "        # load meta_data → could be done in get_item directly to prevent loading each path twice\n",
    "        # if load_meta_data:\n",
    "            # for patch_id in patch_idx:\n",
    "                # patch = xr.open_dataset(os.path.join(dataloader.dataset.root,dataloader.dataset.patch_ids[patch_id]))\n",
    "                # meta_data[\"lat\"].append(patch.latitude.mean().values.item())\n",
    "                # meta_data[\"lon\"].append(patch.longitude.mean().values.item())\n",
    "                # meta_data[\"lwm\"].append(stats.mode(patch.land_water_mask).mode.item())\n",
    "                # meta_data[\"day_night_flag\"].append(patch.day_night_flag.max().values.item())\n",
    "                # meta_data[\"time_of_day\"].append(patch.sensing_stop.dt.hour.values.item())\n",
    "                # meta_data[\"month\"].append(patch.sensing_stop.dt.month.values.item())\n",
    "        \n",
    "        if stop_at_step:\n",
    "            if step==stop_at_step: break\n",
    "\n",
    "    # concat input data\n",
    "    overpass_mask_data = torch.concat(overpass_mask_data)\n",
    "    patch_idx_data = torch.concat(patch_idx_data)\n",
    "    seviri_data = torch.concat(seviri_data)\n",
    "    era5_data = torch.concat(era5_data)\n",
    "    meta_data = torch.concat(meta_data)\n",
    "\n",
    "    # concat cloud cover\n",
    "    dardar_cloud_cover_data = torch.concat(dardar_cloud_cover_data)\n",
    "    y_hat_cloud_cover_data  = torch.concat(y_hat_cloud_cover_data)\n",
    "\n",
    "    # if load_meta_data:\n",
    "        # meta_data[\"day_night_flag\"] = torch.tensor(meta_data[\"day_night_flag\"])\n",
    "        # meta_data[\"lwm\"] = torch.tensor(meta_data[\"lwm\"])\n",
    "        # meta_data[\"lon\"] = torch.tensor(meta_data[\"lon\"])\n",
    "        # meta_data[\"lat\"] = torch.tensor(meta_data[\"lat\"])\n",
    "\n",
    "    # profile data, and cube data are in lists with length |samples|, other data is in concatenated tensors already\n",
    "    eval_data = dict(overpass_mask_data=overpass_mask_data,\n",
    "         patch_idx_data=patch_idx_data,\n",
    "         seviri_data=seviri_data,\n",
    "         era5_data=era5_data,\n",
    "         dardar_profile_data_list=dardar_profile_data_list,\n",
    "         y_hat_profile_data_list=y_hat_profile_data_list,\n",
    "         dardar_cloud_cover_data=dardar_cloud_cover_data,\n",
    "         y_hat_cloud_cover_data =y_hat_cloud_cover_data,\n",
    "         y_hat_cube_data=y_hat_cube_data,\n",
    "         dardar_cube_data=dardar_cube_data,\n",
    "         meta_data=meta_data)\n",
    "    \n",
    "    return eval_data\n",
    "\n",
    "def get_profile_data(eval_data, meta_data_filter=None,target_transform=LogTransform(scaler=1e7),selected_height_levels=None):\n",
    "    \"\"\"Returns concatenated y_hat, dardar data\n",
    "    \n",
    "    args:\n",
    "        eval_data\n",
    "        meta_data_filter (Tuple): (meta data variable index, value)\n",
    "    \n",
    "    \"\"\"\n",
    "    if meta_data_filter:\n",
    "        filtered_idxs = np.where(np.array(eval_data[\"meta_data\"][:,meta_data_filter[0]])==meta_data_filter[1])[0]\n",
    "\n",
    "        y_hat_profile_data_list = [eval_data[\"y_hat_profile_data_list\"][i] for i in filtered_idxs]\n",
    "        dardar_profile_data_list = [eval_data[\"dardar_profile_data_list\"][i] for i in filtered_idxs]\n",
    "    else:\n",
    "        y_hat_profile_data_list = eval_data[\"y_hat_profile_data_list\"]\n",
    "        dardar_profile_data_list = eval_data[\"dardar_profile_data_list\"]\n",
    "\n",
    "    # profile data\n",
    "    dardar_profile_data = torch.concat(dardar_profile_data_list)\n",
    "    y_hat_profile_data = torch.concat(y_hat_profile_data_list)\n",
    "    # convert to original scale → this is what we mainly work with in the evaluation\n",
    "    dardar = target_transform.inverse_transform(dardar_profile_data)\n",
    "    y_hat = target_transform.inverse_transform(y_hat_profile_data)\n",
    "    \n",
    "    if selected_height_levels is not None:\n",
    "        if len(y_hat.shape)==2:\n",
    "            dardar = dardar[:, selected_height_levels]\n",
    "            y_hat = y_hat[:, selected_height_levels]\n",
    "        else:\n",
    "            dardar = dardar[:,:, selected_height_levels]\n",
    "            y_hat = y_hat[:,:, selected_height_levels]\n",
    "    \n",
    "    return y_hat, dardar\n",
    "\n",
    "def get_cube_data(eval_data,target_transform=LogTransform(scaler=1e7)):\n",
    "    \"\"\"todo: add filter and dardar data\"\"\"\n",
    "    y_hat_cube = torch.concat(eval_data[\"y_hat_cube_data\"])\n",
    "    y_hat_cube = target_transform.inverse_transform(y_hat_cube) # transform to original space\n",
    "    # y_hat_cube_flat = einops.rearrange(y_hat_cube, 'b z y x -> (b y x) z ') # rearrange cube data to 2d: n, 256\n",
    "    \n",
    "    return y_hat_cube#, y_hat_cube_flat\n",
    "\n",
    "def run_evaluation(y_hat, \n",
    "                   dardar, \n",
    "                   eval_data, \n",
    "                   exp, \n",
    "                   target_variable=\"iwc\",\n",
    "                   target_transform=LogTransform(scaler=1e7),\n",
    "                   height_levels:np.ndarray=get_height_level_range(1680,16980,step=60),\n",
    "                   cloud_thres=0,\n",
    "                   log_image_kwargs=dict(),\n",
    "                   suffix=\"\"):\n",
    "    \"\"\"runs evaluation given inference/ground truth data and plots eval plots to comet\n",
    "    \n",
    "    todo: alternatively display plots inline\n",
    "    \n",
    "    \"\"\"\n",
    "    if suffix != \"\":\n",
    "        suffix = f\"_{suffix}\"\n",
    "    \n",
    "    if target_variable==\"icnc_5um\":\n",
    "        target_variable=\"nice\"\n",
    "    \n",
    "    level_thickness = np.abs(height_levels[0]-height_levels[1])\n",
    "       \n",
    "    # cloud occurance per height level\n",
    "    hv.extension(\"bokeh\")\n",
    "    df, occurance_p = cloud_occurance_per_height_level(y_hat, dardar, height_levels=height_levels)\n",
    "    log_image(occurance_p, f\"cloud_occurance{suffix}\", **log_image_kwargs,exp_obj=exp)\n",
    "    \n",
    "    # metrics per level\n",
    "    df, metric_p = metrics_per_level(y_hat, dardar, cloud_thres=cloud_thres, height_levels=height_levels, target_transform=target_transform,n_level_aggregation=int(height_levels.shape[0]/16)) # todo better heuristic for n_level_aggregation \n",
    "    log_image(metric_p,f\"performance_metrics_levels{suffix}\", **log_image_kwargs,exp_obj=exp)\n",
    "    \n",
    "\n",
    "    # iwc per height level\n",
    "    y_hat_height_iwc_df, y_hat_height_iwc_plt = iwc_per_height_df(y_hat,color=y_hat_color,plt_q10=False, height_levels=height_levels,target_variable=target_variable)\n",
    "    dardar_height_iwc_df, dardar_height_iwc_plt = iwc_per_height_df(dardar,color=dardar_color,plt_q10=False, height_levels=height_levels,target_variable=target_variable)\n",
    "    p = (y_hat_height_iwc_plt * dardar_height_iwc_plt).opts(fontscale=1.5)\n",
    "    log_image(p, f\"iwc_height{suffix}\", **log_image_kwargs,exp_obj=exp)\n",
    "    \n",
    "    # iwc vs iwc\n",
    "    df, g = iwc_vs_iwc_plt(y_hat, dardar,target_variable=target_variable)\n",
    "    log_image(g, f\"iwc_vs_iwc{suffix}\", **log_image_kwargs,exp_obj=exp)\n",
    "    \n",
    "    \n",
    "    # todo: was displaying only the last plot, same problem for zonal mean count\n",
    "    # for p in height_plts:\n",
    "        # log_image_to_comet(exp, p,image_name=p.fig._suptitle.get_text())\n",
    "    \n",
    "    # metrics \n",
    "    metric_dict = get_metrics(y_hat, dardar, cloud_thres=cloud_thres, target_transform=target_transform)\n",
    "    \n",
    "    if suffix != \"\":\n",
    "        metric_dict = {f\"{key}{suffix}\":value for key,value in metric_dict.items()}\n",
    "    \n",
    "    pprint(metric_dict)\n",
    "    \n",
    "    if suffix == \"\":\n",
    "        # todo implement filter for day/night below\n",
    "        # horizontal cloud cover\n",
    "        all_data_plt, summary_line_plt, hor_cloud_cover_metrics = horizontal_cloud_cover(eval_data[\"dardar_cloud_cover_data\"],eval_data[\"y_hat_cloud_cover_data\"],height_levels=height_levels)\n",
    "        log_image(all_data_plt, f\"horizontal_cloud_cover{suffix}\", **log_image_kwargs,exp_obj=exp)\n",
    "        log_image(summary_line_plt, f\"horizonal_cloud_cover_level_stats{suffix}\", **log_image_kwargs,exp_obj=exp)\n",
    "    \n",
    "    if suffix in [\"\" ,\"_nice\"]:\n",
    "        print(\"calc zonal/iwp means\")\n",
    "        height_levels=get_height_level_range(1680,16980,step=240)[:-1]\n",
    "        # zonal means\n",
    "        fig_zonal = zonal_mean(eval_data[\"y_hat_profile_data_list\"],eval_data[\"dardar_profile_data_list\"],eval_data[\"meta_data\"][:,0],height_levels=height_levels,min_count=250,target_variable=target_variable,target_transform=target_transform)\n",
    "        log_image(fig_zonal, f\"zonal_mean{suffix}\", **log_image_kwargs,exp_obj=exp)\n",
    "\n",
    "        # iwp mean\n",
    "        #fig,dardar_Z,y_hat_Z = iwp_regional_mean(eval_data[\"y_hat_profile_data_list\"],eval_data[\"dardar_profile_data_list\"],eval_data[\"meta_data\"][:,0],eval_data[\"meta_data\"][:,1],level_thickness=level_thickness,height_levels=height_levels,grid_size=4,target_variable=target_variable,target_transform=target_transform)\n",
    "        #log_image(fig, f\"iwp_mean{suffix}\", **log_image_kwargs,exp_obj=exp)\n",
    "    \n",
    "\n",
    "def eval_pipeline(exp_config, \n",
    "                  eval_kwargs, \n",
    "                  meta_data_filter=None, \n",
    "                  dm_experiment_id=\"famous_elk_2849\",\n",
    "                  dm_overwrite_hparams=dm_overwrite_hparams,\n",
    "                  log_image_kwargs=dict(display_inline=True, log_to_overleaf=False, log_to_comet=False),\n",
    "                  run_eval=True):\n",
    "\n",
    "    experiment_id = exp_config[\"experiment_id\"]\n",
    "    print(f\"start eval pipeline for {experiment_id}\")\n",
    "    \n",
    "    dm = get_dm(dm_experiment_id, dm_overwrite_hparams)\n",
    "    val_dataloader = dm.val_dataloader()\n",
    "    print(\"loaded dm\")\n",
    "    \n",
    "    if exp_config[\"type\"]==\"unet_only\":\n",
    "        exp, model = get_trained_experiment(experiment_id,exp_config[\"model_overwrite_hparams\"])\n",
    "    elif exp_config[\"type\"]==\"unet_disc\":\n",
    "        exp, model, conf = load_discriminator_model(experiment_id)\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    model = model.to(torch.device(\"cuda\"))\n",
    "    model.eval()\n",
    "    \n",
    "    eval_data = create_inference_data(model, val_dataloader, **eval_kwargs)\n",
    "    #todo implement loop for meta data filter\n",
    "    y_hat, dardar = get_profile_data(eval_data, meta_data_filter=meta_data_filter,target_transform=dm.target_transform)\n",
    "    y_hat_night, dardar_night = get_profile_data(eval_data, meta_data_filter=(3,1),target_transform=dm.target_transform)\n",
    "    y_hat_day, dardar_day = get_profile_data(eval_data, meta_data_filter=(3,0),target_transform=dm.target_transform)\n",
    "    \n",
    "    if eval_kwargs[\"store_3d_data\"]:\n",
    "        y_hat_cube = get_cube_data(eval_data,target_transform=dm.target_transform)\n",
    "    else:\n",
    "        y_hat_cube = None#, None\n",
    "    print(\"generated eval data\")\n",
    "    \n",
    "    # define height levels for eval\n",
    "    if dm.fold_to_level_thickness:\n",
    "        level_thickness = 60*dm.fold_to_level_thickness\n",
    "    else:\n",
    "        level_thickness = 60\n",
    "    height_levels = get_height_level_range(dm.height_levels[1],dm.height_levels[0],level_thickness)\n",
    "    height_levels = height_levels[-y_hat.shape[1]:] # height levels has to have the same shape as predictions / cut off highest hight if necessary\n",
    "    \n",
    "    target_variable = \"iwc\" if dm.target_variable==\"iwc\" else \"nice\"\n",
    "    \n",
    "    # run evaluation plot pipeline\n",
    "    if run_eval:\n",
    "        try:\n",
    "            run_evaluation(y_hat=y_hat,dardar=dardar,eval_data=eval_data,exp=exp,target_variable=target_variable,target_transform=dm.target_transform,height_levels=height_levels, log_image_kwargs=log_image_kwargs)\n",
    "            print(\"saved plots to comet\")\n",
    "            run_evaluation(y_hat=y_hat_day,dardar=dardar_day,eval_data=eval_data,exp=exp,target_variable=target_variable,target_transform=dm.target_transform,height_levels=height_levels, log_image_kwargs=log_image_kwargs,suffix=\"day\")\n",
    "            print(\"saved day plots to comet\")\n",
    "            run_evaluation(y_hat=y_hat_night,dardar=dardar_night,eval_data=eval_data,exp=exp,target_variable=target_variable,target_transform=dm.target_transform,height_levels=height_levels, log_image_kwargs=log_image_kwargs,suffix=\"night\")\n",
    "            print(\"saved night plots to comet\")\n",
    "        except BaseException as ex:\n",
    "            print(ex)\n",
    "    \n",
    "    return exp, dm, model, eval_data, y_hat, dardar, y_hat_cube\n",
    "\n",
    "def log_image(p, \n",
    "              image_name, \n",
    "              display_inline=True,\n",
    "              log_to_overleaf=False, \n",
    "              overleaf_dir=\"\",\n",
    "              log_to_comet=False, \n",
    "              exp_obj=None, \n",
    "              comet_log_kwargs=dict(overwrite=True)):\n",
    "    if log_to_comet:\n",
    "        assert exp_obj is not None, \"provide comet experiment object\"\n",
    "        log_image_to_comet(exp_obj,p,image_name,log_kwargs=comet_log_kwargs)\n",
    "        \n",
    "    if log_to_overleaf:\n",
    "        fpath = os.path.join(overleaf_dir, f\"{image_name}.png\")\n",
    "        #hvplot\n",
    "        if \"holoviews\" in str(type(p)):\n",
    "            hv.save(p,fpath,dpi=600)\n",
    "        # matplotlib\n",
    "        else:\n",
    "        # elif isinstance(p,plt.Figure):\n",
    "            p.savefig(fpath,dpi=600)\n",
    "            if isinstance(p,plt.Figure):\n",
    "                plt.close(p)\n",
    "            else:\n",
    "                # seaborn\n",
    "                plt.close(p.fig)\n",
    "    if display_inline:    \n",
    "        if isinstance(p, hv.core.overlay.Overlay) or isinstance(p,hv.core.overlay.NdOverlay):\n",
    "            display(p)\n",
    "        else:\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2210ef34-87ae-4401-9c1c-4a829de03435",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_kwargs_commons = {'var_range': \n",
    "                      {'iwc': (1e-07, 0.001), \n",
    "                       'nice': (1e2, 1e6)},\n",
    "                      'var_range_log': \n",
    "                      {'iwc': (-7, -3), \n",
    "                       'nice': (2, 6)},\n",
    "                      'axis_title':\n",
    "                       {'iwc': r'IWC [kg m$^{-3}$]',\n",
    "                        'nice': r'Nice [m$^{-3}$]'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bd9017-8f1b-42ed-a109-782fffbad4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4960842-1bcb-4f17-9f12-7321f8d577f1",
   "metadata": {},
   "source": [
    "### Get patch_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c03a35c-228c-4ade-bb6f-a7185aa17aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../helper_files\"\n",
    "with open(os.path.join(data_dir,\"train_pids.json\"), 'r') as file:\n",
    "    train_patch_ids = json.load(file)\n",
    "            \n",
    "with open(os.path.join(data_dir,\"val_pids.json\"), 'r') as file:\n",
    "    val_patch_ids = json.load(file)\n",
    "    \n",
    "with open(os.path.join(data_dir,\"test_pids.json\"), 'r') as file:\n",
    "    val_patch_ids = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4ed7a5-45a7-4ce0-8212-871ae6c905f8",
   "metadata": {},
   "source": [
    "### Load model & data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade2e121-a5c1-45e3-bc5a-bdfc8ff8e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo set `data_dir` to path containing TrainingData\n",
    "model, dm = load_model(data_dir=\"/net/n2o/wolke_scratch2/kjeggle/VerticalCloud/Nice128\",\n",
    "                   train_patch_ids=[],\n",
    "                   val_patch_ids=val_patch_ids,\n",
    "                   model_conf_filepath=\"../model_configs/ice_cloud_net_conf.yaml\",\n",
    "                   model_checkpoint_dir=\"../model_checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48821b7e-0e46-4cc5-8617-c0f504247841",
   "metadata": {},
   "source": [
    "### Run evaluation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d8e949-3596-46ac-bd4b-822b3a17157f",
   "metadata": {},
   "source": [
    "#### Get eval data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7a2c1-5192-421c-980c-08b29e904ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_kwargs = dict(store_3d_data=False,load_meta_data = True,split=\"val\",stop_at_step=250)\n",
    "meta_data_filter = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa0cf3b-e00d-4638-909c-e633c6f7dd0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model.eval()\n",
    "\n",
    "eval_data = create_inference_data(model, dm.val_dataloader(), **eval_kwargs)\n",
    "#todo implement loop for meta data filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6aa024-1b61-4c51-bd3d-71a3a6f3371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "overpass_length = torch.sum(eval_data[\"overpass_mask_data\"],dim=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee5bd1-fbe5-46f5-a7e9-d2c7020aaf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "overpass_length = overpass_length[overpass_length>256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c1b59-875c-4c85-a599-b924100de9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dm.fold_to_level_thickness:\n",
    "    level_thickness = 60*dm.fold_to_level_thickness\n",
    "else:\n",
    "    level_thickness = 60\n",
    "height_levels = get_height_level_range(dm.height_levels[1],dm.height_levels[0],level_thickness)\n",
    "height_levels = height_levels[-model.out_channels:] # height levels has to have the same shape as predictions / cut off highest hight if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71a48d9-1913-4d27-93ee-633e3dcfe13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_height_levels = height_levels > 3800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66812193-a709-46f2-98af-200aa20601ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_hat_iwc, dardar_iwc = get_profile_data(eval_data, meta_data_filter=None,target_transform=LogTransform(scaler=1e7),selected_height_levels=selected_height_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcbae93-cc83-436e-a6b7-a732d6b789a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_iwc = y_hat_iwc[:,0]\n",
    "dardar_iwc = dardar_iwc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29652c73-ddb0-4254-9609-d8060cdab33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_night_iwc, dardar_night_iwc = get_profile_data(eval_data, meta_data_filter=(3,1),target_transform=LogTransform(scaler=1e7),selected_height_levels=selected_height_levels)\n",
    "y_hat_day_iwc, dardar_day_iwc = get_profile_data(eval_data, meta_data_filter=(3,0),target_transform=LogTransform(scaler=1e7),selected_height_levels=selected_height_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c03246a-d002-4154-8ec4-585f0ed2538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_night_iwc = y_hat_night_iwc[:,0]\n",
    "dardar_night_iwc = dardar_night_iwc[:,0]\n",
    "\n",
    "y_hat_day_iwc = y_hat_day_iwc[:,0]\n",
    "dardar_day_iwc = dardar_day_iwc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c573ec04-1aeb-4043-ba7e-2183f7350a0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get y_hat_cube n, z,y,x\n",
    "if eval_kwargs[\"store_3d_data\"]:\n",
    "    y_hat_cube_iwc = torch.concat(eval_data[\"y_hat_cube_data\"])\n",
    "    y_hat_cube_iwc = dm.target_transform.inverse_transform(y_hat_cube_iwc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80eeff8-d6ff-48bb-8a8f-7df898981a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nice_logtrans = LogTransform(scaler=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350631ce-c239-4c10-925e-cc67cee10bd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_hat_nice, dardar_nice = get_profile_data(eval_data, meta_data_filter=meta_data_filter,target_transform=nice_logtrans,selected_height_levels=selected_height_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c736fd01-c416-431b-90d3-13f616c45f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_nice = y_hat_nice[:,1]\n",
    "dardar_nice = dardar_nice[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4992a2-6449-4c02-ab2e-9c8de8efe8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_night_nice, dardar_night_nice = get_profile_data(eval_data, meta_data_filter=(3,1),target_transform=nice_logtrans,selected_height_levels=selected_height_levels)\n",
    "y_hat_day_nice, dardar_day_nice = get_profile_data(eval_data, meta_data_filter=(3,0),target_transform=nice_logtrans,selected_height_levels=selected_height_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d972617c-1a52-4e1f-b2ec-6258418db3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_night_nice = y_hat_night_nice[:,1]\n",
    "dardar_night_nice = dardar_night_nice[:,1]\n",
    "\n",
    "y_hat_day_nice = y_hat_day_nice[:,1]\n",
    "dardar_day_nice = dardar_day_nice[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5472a5e9-2956-43b9-9b85-5f7288b7d9d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get y_hat_cube n, z,y,x\n",
    "if eval_kwargs[\"store_3d_data\"]:\n",
    "    y_hat_cube_nice = torch.concat(eval_data[\"y_hat_cube_data\"])\n",
    "    y_hat_cube_nice = dm.target_transform.inverse_transform(y_hat_cube_nice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed6dd89-bef9-4721-bb5b-8134b488c594",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = y_hat_iwc\n",
    "dardar = dardar_iwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133fb8b9-9f1c-41a4-ac59-aa8fbbf41ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_night = y_hat_night_iwc\n",
    "dardar_night = dardar_night_iwc\n",
    "\n",
    "y_hat_day = y_hat_day_iwc\n",
    "dardar_day = dardar_day_iwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5edd2c9-a970-4c4d-8dc1-5db837936610",
   "metadata": {},
   "outputs": [],
   "source": [
    "height_levels = height_levels[selected_height_levels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0be5640-7f55-4c39-9515-005ea2947f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_percentage = (dardar>0).sum() / (dardar>=0).sum()\n",
    "print(f\"% of cloudy pixels {cloud_percentage:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000c2c9a-0f8d-459a-b501-26a5fb812902",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_percentage = (dardar>0).sum() / (dardar>=0).sum()\n",
    "print(f\"% of cloudy pixels {cloud_percentage:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b363c648-4255-4968-8329-ca749c720c26",
   "metadata": {},
   "source": [
    "### Run plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcfc7eb-a5c3-41e6-a5be-776b2b9bee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8390f9be-96ea-40df-ad9e-94bb159654d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# iwc \n",
    "run_evaluation(y_hat_iwc,dardar_iwc,eval_data,exp=None,height_levels=height_levels,target_transform=LogTransform(scaler=1e7),target_variable=\"iwc\",log_image_kwargs=dict(display_inline=True, log_to_overleaf=False, log_to_comet=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ae7cb3-6af9-4346-8921-93c7df862214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# iwc night\n",
    "run_evaluation(y_hat_night_iwc,dardar_night_iwc,eval_data,exp=None,height_levels=height_levels,target_transform=LogTransform(scaler=1e7),target_variable=\"iwc\",suffix=\"night\",log_image_kwargs=dict(display_inline=True, log_to_overleaf=False, log_to_comet=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b9a9db-b7fa-4ee9-bf1e-3cbc2d124c44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# iwc day\n",
    "run_evaluation(y_hat_day_iwc,dardar_day_iwc,eval_data,exp=None,height_levels=height_levels,target_transform=LogTransform(scaler=1e7),target_variable=\"iwc\",suffix=\"day\",log_image_kwargs=dict(display_inline=True, log_to_overleaf=False, log_to_comet=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681b4c41-281a-4cdc-a97a-ac8e0c4cb16c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# icnc \n",
    "run_evaluation(y_hat_nice,dardar_nice,eval_data,exp=None,height_levels=height_levels,target_transform=nice_logtrans,target_variable=\"nice\",suffix=\"nice\",log_image_kwargs=dict(display_inline=True, log_to_overleaf=False, log_to_comet=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a738e6ae-a848-4dd6-bbf3-1a0236968ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# icnc day\n",
    "run_evaluation(y_hat_night_nice,dardar_night_nice,eval_data,exp=None,height_levels=height_levels,target_transform=nice_logtrans,target_variable=\"nice\",suffix=\"nice_night\",log_image_kwargs=dict(display_inline=True, log_to_overleaf=False, log_to_comet=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473865cf-3b5c-479b-ac70-5442dd0062ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# icnc _night\n",
    "run_evaluation(y_hat_day_nice,dardar_day_nice,eval_data,exp=None,height_levels=height_levels,target_transform=nice_logtrans,target_variable=\"nice\",suffix=\"nice_day\",log_image_kwargs=dict(display_inline=True, log_to_overleaf=False, log_to_comet=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_xarray]",
   "language": "python",
   "name": "conda-env-pytorch_xarray-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
